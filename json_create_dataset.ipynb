{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKSPACE_PATH = 'workspace'\n",
    "ANNOTATION_PATH = WORKSPACE_PATH+'/annotations'\n",
    "IMAGE_PATH = WORKSPACE_PATH+'/images'\n",
    "MODEL_PATH = WORKSPACE_PATH+'/models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Tristan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lade JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def json_read(data_path):\n",
    "    with open(data_path, 'r') as data:\n",
    "        data = json.load(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON_NAME = '/instances_default.json'\n",
    "DATA_PATH = ANNOTATION_PATH+'/original'+ JSON_NAME\n",
    "data = json_read(DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bennene Bilder im Verzeichnis images und in der Annotationsdatei um"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def rename_images(IMAGE_PATH):\n",
    "    index=1\n",
    "    img  = os.listdir(IMAGE_PATH)\n",
    "    for i in img:\n",
    "        new_name = f\"{index:012d}\"+'.jpeg'\n",
    "        old_path = os.path.join(IMAGE_PATH, i)\n",
    "        new_path = os.path.join(IMAGE_PATH, new_name)\n",
    "        os.rename(old_path, new_path)\n",
    "        index += 1\n",
    "\n",
    "def rename_images_in_annotation():\n",
    "    index=1\n",
    "    for img in data['images']:\n",
    "        new_name = f\"{index:012d}\"+'.jpeg'\n",
    "        img['file_name'] = new_name \n",
    "        index += 1\n",
    "\n",
    "rename_images(IMAGE_PATH+'/original')\n",
    "rename_images_in_annotation()\n",
    "\n",
    "with open(ANNOTATION_PATH+'/original' + JSON_NAME, 'w') as destination_file:\n",
    "    json.dump(data, destination_file, indent=2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "bounding_boxes = []\n",
    "labels = []\n",
    "image_height = 1366\n",
    "image_width = 768\n",
    "\n",
    "for image in data['images']:\n",
    "    one_image = []\n",
    "    one_image.append(image['id'])\n",
    "    one_image.append(image['file_name'])\n",
    "    images.append(one_image)\n",
    "\n",
    "for annotation in data['annotations']:\n",
    "    one_box = []\n",
    "    box_and_id = []\n",
    "    one_box.append(annotation['bbox'])\n",
    "    bounding_boxes.append(one_box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_imgaes(input_path, output_path, height, width):\n",
    "    image =  cv2.imread(input_path)\n",
    "    resized_image = cv2.resize(image, (height, width))\n",
    "    cv2.imwrite(output_path, resized_image)    \n",
    "\n",
    "def resize_bounding_boxes_and_area(input_path,bounding_boxes, height, width):\n",
    "    image =  cv2.imread(input_path)\n",
    "    height_ratio = height / image.shape[0]\n",
    "    width_ratio = width / image.shape[1]\n",
    "    \n",
    "    for box in bounding_boxes:\n",
    "        resized_boxes = []\n",
    "        resized_area = 0\n",
    "\n",
    "        x = np.round(box[0]*width_ratio,2)\n",
    "        y = np.round(box[1]*height_ratio,2)\n",
    "        x_width = np.round(box[2]*width_ratio,2)\n",
    "        y_height = np.round(box[3]*height_ratio,2)\n",
    "        resized_boxes.append([x, y, x_width, y_height])\n",
    "\n",
    "        resized_area = x_width * y_height\n",
    "\n",
    "    return x, y, x_width, y_height, resized_area\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "new_height = 224\n",
    "new_width = 224\n",
    "OUTPUT_PATH_IMAGES = IMAGE_PATH + '/' + str(new_height) + 'x' + str(new_width)\n",
    "OUTPUTPATH_ANNOTATIONS = ANNOTATION_PATH + '/' + str(new_height) + 'x' + str(new_width)\n",
    "\n",
    "os.makedirs(OUTPUT_PATH_IMAGES, exist_ok=True)\n",
    "os.makedirs(OUTPUTPATH_ANNOTATIONS, exist_ok=True)\n",
    "\n",
    "#Copy JSON Doc in new ANNOTATIONPATH\n",
    "with open(DATA_PATH, 'r') as source_file:\n",
    "    data = json.load(source_file)\n",
    "with open(OUTPUTPATH_ANNOTATIONS + JSON_NAME, 'w') as destination_file:\n",
    "    json.dump(data, destination_file, indent=2)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resize Images\n",
    "for img in range(len(images)):\n",
    "    resize_imgaes(IMAGE_PATH+'/original'+ '/' +images[img][1], OUTPUT_PATH_IMAGES+ '/' +images[img][1], new_height, new_width)\n",
    "\n",
    "\n",
    "with open(OUTPUTPATH_ANNOTATIONS + JSON_NAME, 'r') as json_data:\n",
    "    data = json.load(json_data)\n",
    "\n",
    "\n",
    "#Resize BBox and area\n",
    "for i, annotation in enumerate(data['annotations']):\n",
    "    calculate_resize = resize_bounding_boxes_and_area(IMAGE_PATH+'/original'+ '/' +images[0][1],bounding_boxes[i], new_height, new_width)\n",
    "    new_box = calculate_resize[0],calculate_resize[1],calculate_resize[2],calculate_resize[3]\n",
    "    new_area = calculate_resize[4]\n",
    "    annotation['bbox'] = new_box\n",
    "    annotation['area'] = new_area\n",
    "\n",
    "#Change height/width\n",
    "for image in data['images']:\n",
    "    image['height']=new_height\n",
    "    image['width']=new_width\n",
    "\n",
    "with open(OUTPUTPATH_ANNOTATIONS + JSON_NAME, 'w') as destination_file:\n",
    "    json.dump(data, destination_file, indent=2)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotation 697\n",
      "{'image_path': 'workspace/images/224x224/000000000001.jpeg', 'bbox': [11.48, 60.11, 151.89, 110.05], 'class_id': 1}\n",
      "workspace/images/224x224/000000000001.jpeg\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "images = data['images']\n",
    "annotations = data['annotations']\n",
    "categories = data['categories']\n",
    "\n",
    "dataset = []\n",
    "\n",
    "for annotation in annotations:\n",
    "    image_info = next(image for image in images if image['id'] == annotation['image_id'])\n",
    "    image_path = OUTPUT_PATH_IMAGES + '/'+ image_info['file_name']\n",
    "    bbox = annotation['bbox']  # [x, y, width, height]\n",
    "    class_id = annotation['category_id']\n",
    "\n",
    "    xmin, ymin, width, height = bbox\n",
    "    xmax, ymax = xmin + width, ymin + height\n",
    "\n",
    "    dataset.append({\n",
    "        'image_path': image_path,\n",
    "        'bbox': [xmin, ymin, xmax, ymax],\n",
    "        'class_id': class_id\n",
    "    })\n",
    "\n",
    "print('Annotation',len(dataset))\n",
    "da =dataset[0]\n",
    "print(dataset[0])\n",
    "print(da['image_path'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_file_list(directory):\n",
    "    file_list = []\n",
    "    for filename in os.listdir(directory):\n",
    "        full_path = os.path.join(directory, filename)\n",
    "        normalized_path = os.path.normpath(full_path)\n",
    "        normalized_path = normalized_path.replace(os.path.sep, '/')\n",
    "        if os.path.isfile(normalized_path):\n",
    "            file_list.append(normalized_path)\n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'class_label': array([ 1,  2,  3,  3, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23,\n",
      "       24, 25, 26, 27, 28, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 39,\n",
      "       39, 39,  7,  8,  5,  6,  9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n",
      "       21, 22, 23, 24, 25, 26, 27, 28, 28, 30, 31, 32, 33, 34, 35, 36, 37,\n",
      "       38, 39, 39, 39, 39,  7,  8,  5,  6,  9, 10,  2,  3,  4, 11, 12, 13,\n",
      "       14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 28, 30,\n",
      "       31, 32, 33, 34, 35, 36, 37, 38, 39, 39, 39, 39,  7,  8,  5,  6,  9,\n",
      "       10,  2,  3,  4, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23,\n",
      "       24, 25, 26, 27, 28, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 39,\n",
      "       39, 39,  7,  8,  5,  6,  9, 10,  2,  3,  4, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 28, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 39, 39, 39,  7,  8,  5,  6,  9, 10,  2,  3,\n",
      "        4, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26,\n",
      "       27, 28, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 39, 39, 39,  7,\n",
      "        8,  5,  6,  9, 10,  2,  3,  4, 11, 12, 13, 14, 15, 16, 17, 18, 19,\n",
      "       20, 21, 22, 23, 24, 25, 26, 27, 28, 28, 30, 31, 32, 33, 34, 35, 36,\n",
      "       37, 38, 39, 39, 39, 39,  7,  8,  5,  6,  9, 10,  2,  3,  4, 11, 12,\n",
      "       13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 28,\n",
      "       30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 39, 39, 39,  7,  8,  5,  6,\n",
      "        9, 10,  2,  3,  4, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22,\n",
      "       23, 24, 25, 26, 27, 28, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39,\n",
      "       39, 39, 39,  7,  8,  5,  6,  9, 10,  2,  3,  4, 11, 12, 13, 14, 15,\n",
      "       16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 28, 30, 31, 32,\n",
      "       33, 34, 35, 36, 37, 38, 39, 39, 39, 39,  7,  8,  5,  6,  9, 10,  2,\n",
      "        3,  4, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25,\n",
      "       26, 27, 28, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 39, 39, 39,\n",
      "        7,  8,  5,  6,  9, 10,  2,  3,  4, 11, 12, 13, 14, 15, 16, 17, 18,\n",
      "       19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 28, 30, 31, 32, 33, 34, 35,\n",
      "       36, 37, 38, 39, 39, 39, 39,  7,  8,  5,  6,  9, 10,  2,  3,  4, 11,\n",
      "       12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28,\n",
      "       28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 39, 39, 39,  7,  8,  5,\n",
      "        6,  9, 10,  2,  3,  4, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21,\n",
      "       22, 23, 24, 25, 26, 27, 28, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38,\n",
      "       39, 39, 39, 39,  7,  8,  5,  6,  9, 10,  2,  3,  4, 11, 12, 13, 14,\n",
      "       15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 28, 30, 31,\n",
      "       32, 33, 34, 35, 36, 37, 38, 39, 39, 39, 39,  7,  8,  5,  6,  9, 10,\n",
      "        2,  3,  4, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24,\n",
      "       25, 26, 27, 28, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 39, 39,\n",
      "       39,  7,  8,  5,  6,  9, 10,  2,  3,  4, 11, 12, 13, 14, 15, 16, 17,\n",
      "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 28, 30, 31, 32, 33, 34,\n",
      "       35, 36, 37, 38, 39, 39, 39, 39,  7,  8,  5,  6,  9, 10,  2,  3,  4]), 'bounding_box': array([[ 11.48,  60.11, 140.41,  49.94],\n",
      "       [ 13.34,  70.43, 134.04,   6.77],\n",
      "       [ 13.03,  77.53, 134.04,   6.77],\n",
      "       ...,\n",
      "       [ 15.96,  69.79, 133.24,   7.47],\n",
      "       [ 15.54,  78.22, 133.66,   6.98],\n",
      "       [ 15.11,  85.21, 134.09,   7.47]], dtype=float32)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ntestTargets = {\\n    \"class_label\": testLabels,\\n    \"bounding_box\": testBBoxes\\n}\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def convert_annotations_to_array(coco_data, num_classes=41):\n",
    "    annotations = coco_data['annotations']\n",
    "    num_annotations = len(annotations)\n",
    "\n",
    "    # Initialize empty arrays for bounding boxes and labels\n",
    "    bounding_boxes = np.zeros((num_annotations, 4), dtype=np.float32)\n",
    "    labels = np.zeros((num_annotations,), dtype=np.int32)\n",
    "\n",
    "    for i, annotation in enumerate(annotations):\n",
    "        # Extract bounding box coordinates\n",
    "        bbox = annotation['bbox']\n",
    "        bounding_boxes[i] = np.array([bbox[0], bbox[1], bbox[2], bbox[3]], dtype=np.float32)\n",
    "\n",
    "        # Extract class label\n",
    "        labels[i] = annotation['category_id']\n",
    "\n",
    "    return bounding_boxes, labels\n",
    "\n",
    "\n",
    "bounding_boxes, labels = convert_annotations_to_array(data)\n",
    "\n",
    "\n",
    "trainTargets = {\n",
    "    \"class_label\": labels,\n",
    "    \"bounding_box\": bounding_boxes\n",
    "}\n",
    "print(trainTargets)\n",
    "\"\"\"\n",
    "testTargets = {\n",
    "    \"class_label\": testLabels,\n",
    "    \"bounding_box\": testBBoxes\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17, 224, 224, 3)\n",
      "(17, 41, 4)\n",
      "(17, 41)\n"
     ]
    }
   ],
   "source": [
    "directory_path = OUTPUT_PATH_IMAGES\n",
    "files = get_file_list(directory_path)\n",
    "\n",
    "\n",
    "bbox_list=[]\n",
    "class_id_list=[]\n",
    "image_path_list=[]\n",
    "\n",
    "for k in files:\n",
    "    ann_bbox=[]\n",
    "    ann_class_id=[]\n",
    "    for i in dataset:\n",
    "        d = i\n",
    "        if d['image_path'] == k:\n",
    "            ann_bbox.append(d['bbox'])\n",
    "            #ann_bbox.append(d['class_id'])\n",
    "            ann_class_id.append(d['class_id'])\n",
    "    \n",
    "    image = cv2.imread(d['image_path'])\n",
    "    image = image / 255.0 \n",
    "\n",
    "        \n",
    "    bbox_list.append(ann_bbox)\n",
    "    class_id_list.append(ann_class_id)\n",
    "    image_path_list.append(image)\n",
    "\n",
    "images_data = np.array(image_path_list)\n",
    "bounding_boxes = np.array(bbox_list)\n",
    "class_labels = np.array(class_id_list)\n",
    "\n",
    "print(images_data.shape)\n",
    "print(bounding_boxes.shape)\n",
    "print(class_labels.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n",
      "WARNING:tensorflow:From c:\\Users\\Tristan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Tristan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import keras_cv\n",
    "\n",
    "base_model = keras_cv.models.MobileNetV3Backbone.from_preset(\n",
    "    \"mobilenet_v3_large_imagenet\",\n",
    "    load_weights=False,\n",
    ")\n",
    "#Freeze base model\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New Top Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1/1 [==============================] - 6s 6s/step - loss: 6528.3394 - class_id_loss: 3.7136 - bbox_loss: 6524.6260 - class_id_accuracy: 0.0000e+00 - val_loss: 7094.2456 - val_class_id_loss: 3.7142 - val_bbox_loss: 7090.5312 - val_class_id_accuracy: 0.0000e+00\n",
      "Epoch 2/20\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 6528.2163 - class_id_loss: 3.7122 - bbox_loss: 6524.5044 - class_id_accuracy: 0.1538 - val_loss: 7094.1157 - val_class_id_loss: 3.7147 - val_bbox_loss: 7090.4009 - val_class_id_accuracy: 0.0000e+00\n",
      "Epoch 3/20\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 6528.0938 - class_id_loss: 3.7107 - bbox_loss: 6524.3828 - class_id_accuracy: 0.1538 - val_loss: 7093.9849 - val_class_id_loss: 3.7153 - val_bbox_loss: 7090.2695 - val_class_id_accuracy: 0.0000e+00\n",
      "Epoch 4/20\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 6527.9697 - class_id_loss: 3.7093 - bbox_loss: 6524.2607 - class_id_accuracy: 0.1538 - val_loss: 7093.8550 - val_class_id_loss: 3.7159 - val_bbox_loss: 7090.1392 - val_class_id_accuracy: 0.0000e+00\n",
      "Epoch 5/20\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 6527.8467 - class_id_loss: 3.7079 - bbox_loss: 6524.1387 - class_id_accuracy: 0.1538 - val_loss: 7093.7231 - val_class_id_loss: 3.7165 - val_bbox_loss: 7090.0068 - val_class_id_accuracy: 0.0000e+00\n",
      "Epoch 6/20\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 6527.7231 - class_id_loss: 3.7065 - bbox_loss: 6524.0166 - class_id_accuracy: 0.1538 - val_loss: 7093.5933 - val_class_id_loss: 3.7171 - val_bbox_loss: 7089.8760 - val_class_id_accuracy: 0.0000e+00\n",
      "Epoch 7/20\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 6527.6006 - class_id_loss: 3.7051 - bbox_loss: 6523.8955 - class_id_accuracy: 0.1538 - val_loss: 7093.4624 - val_class_id_loss: 3.7177 - val_bbox_loss: 7089.7446 - val_class_id_accuracy: 0.0000e+00\n",
      "Epoch 8/20\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 6527.4766 - class_id_loss: 3.7037 - bbox_loss: 6523.7729 - class_id_accuracy: 0.1538 - val_loss: 7093.3320 - val_class_id_loss: 3.7183 - val_bbox_loss: 7089.6138 - val_class_id_accuracy: 0.0000e+00\n",
      "Epoch 9/20\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 6527.3535 - class_id_loss: 3.7023 - bbox_loss: 6523.6514 - class_id_accuracy: 0.1538 - val_loss: 7093.2007 - val_class_id_loss: 3.7189 - val_bbox_loss: 7089.4819 - val_class_id_accuracy: 0.0000e+00\n",
      "Epoch 10/20\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 6527.2295 - class_id_loss: 3.7009 - bbox_loss: 6523.5288 - class_id_accuracy: 0.1538 - val_loss: 7093.0703 - val_class_id_loss: 3.7195 - val_bbox_loss: 7089.3511 - val_class_id_accuracy: 0.0000e+00\n",
      "Epoch 11/20\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 6527.1069 - class_id_loss: 3.6995 - bbox_loss: 6523.4072 - class_id_accuracy: 0.1538 - val_loss: 7092.9399 - val_class_id_loss: 3.7201 - val_bbox_loss: 7089.2197 - val_class_id_accuracy: 0.0000e+00\n",
      "Epoch 12/20\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 6526.9834 - class_id_loss: 3.6981 - bbox_loss: 6523.2847 - class_id_accuracy: 0.1538 - val_loss: 7092.8096 - val_class_id_loss: 3.7207 - val_bbox_loss: 7089.0889 - val_class_id_accuracy: 0.0000e+00\n",
      "Epoch 13/20\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 6526.8604 - class_id_loss: 3.6967 - bbox_loss: 6523.1636 - class_id_accuracy: 0.1538 - val_loss: 7092.6782 - val_class_id_loss: 3.7212 - val_bbox_loss: 7088.9570 - val_class_id_accuracy: 0.0000e+00\n",
      "Epoch 14/20\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 6526.7363 - class_id_loss: 3.6953 - bbox_loss: 6523.0410 - class_id_accuracy: 0.1538 - val_loss: 7092.5483 - val_class_id_loss: 3.7218 - val_bbox_loss: 7088.8267 - val_class_id_accuracy: 0.0000e+00\n",
      "Epoch 15/20\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 6526.6128 - class_id_loss: 3.6939 - bbox_loss: 6522.9194 - class_id_accuracy: 0.1538 - val_loss: 7092.4175 - val_class_id_loss: 3.7224 - val_bbox_loss: 7088.6948 - val_class_id_accuracy: 0.0000e+00\n",
      "Epoch 16/20\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 6526.4897 - class_id_loss: 3.6925 - bbox_loss: 6522.7974 - class_id_accuracy: 0.1538 - val_loss: 7092.2861 - val_class_id_loss: 3.7230 - val_bbox_loss: 7088.5630 - val_class_id_accuracy: 0.0000e+00\n",
      "Epoch 17/20\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 6526.3662 - class_id_loss: 3.6911 - bbox_loss: 6522.6753 - class_id_accuracy: 0.1538 - val_loss: 7092.1558 - val_class_id_loss: 3.7236 - val_bbox_loss: 7088.4321 - val_class_id_accuracy: 0.0000e+00\n",
      "Epoch 18/20\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 6526.2432 - class_id_loss: 3.6897 - bbox_loss: 6522.5542 - class_id_accuracy: 0.1538 - val_loss: 7092.0254 - val_class_id_loss: 3.7242 - val_bbox_loss: 7088.3013 - val_class_id_accuracy: 0.0000e+00\n",
      "Epoch 19/20\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 6526.1201 - class_id_loss: 3.6883 - bbox_loss: 6522.4316 - class_id_accuracy: 0.1538 - val_loss: 7091.8950 - val_class_id_loss: 3.7248 - val_bbox_loss: 7088.1704 - val_class_id_accuracy: 0.0000e+00\n",
      "Epoch 20/20\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 6525.9971 - class_id_loss: 3.6869 - bbox_loss: 6522.3101 - class_id_accuracy: 0.1538 - val_loss: 7091.7646 - val_class_id_loss: 3.7254 - val_bbox_loss: 7088.0391 - val_class_id_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2351909aed0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "inputs = keras.Input(shape=(new_height, new_width, 3))\n",
    "x = base_model(inputs, training=False)\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "class_id_output = keras.layers.Dense(41, activation='softmax', name='class_id')(x)\n",
    "\n",
    "bbox_output = keras.layers.Dense(4, name='bbox')(x)\n",
    "model = keras.Model(inputs, [class_id_output, bbox_output]) \n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss={'class_id': 'sparse_categorical_crossentropy', 'bbox': 'mean_squared_error'},#Regression f√ºr bbox\n",
    "    metrics={'class_id': 'accuracy'}\n",
    ")\n",
    "\n",
    "model.fit(images_data,  {\"class_id\": labels,\"bbox\": bounding_boxes}, epochs=20, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "[[0.02410258 0.02508543 0.02508543 0.02508567 0.02410258 0.02410258\n",
      "  0.02410258 0.02410258 0.02410258 0.02410258 0.02410258 0.02508543\n",
      "  0.02508543 0.02508543 0.02508543 0.02508543 0.02508543 0.02508543\n",
      "  0.02508543 0.02508543 0.02410258 0.02410258 0.02410258 0.02410258\n",
      "  0.02410258 0.02410258 0.02410258 0.02410258 0.02410258 0.02410258\n",
      "  0.02410258 0.02410258 0.02410258 0.02410258 0.02410258 0.02410258\n",
      "  0.02410258 0.02410258 0.02410258 0.02410258 0.02410258]]\n"
     ]
    }
   ],
   "source": [
    "img = OUTPUT_PATH_IMAGES + '/000000000001.jpeg'\n",
    "img = image.load_img(OUTPUT_PATH_IMAGES + '/000000000001.jpeg', target_size=(new_height, new_width))\n",
    "img_array = image.img_to_array(img)\n",
    "img_array = np.expand_dims(img_array, axis=0) \n",
    "img_array /= 255.0  \n",
    "boxPreds, labelPreds = model.predict(img_array)\n",
    "print(boxPreds)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
